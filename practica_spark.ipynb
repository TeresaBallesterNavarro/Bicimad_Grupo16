{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6365ace9",
   "metadata": {},
   "source": [
    "# Importamos librerias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f81a4f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb0091fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql import SparkSession #Para crear una sesión de Spark\n",
    "from pyspark.sql.types import *  \n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from collections import Counter #Para contar la frecuencia de los elementos de una lista\n",
    "import json # Datos en formato JSON, pues los datos de BICIMAD vienen en ese formato\n",
    "#from pprint import pprint # Para imprimir de manera legible los resultados\n",
    "import datetime # Para poder trabajar con fechas \n",
    "import sys\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa03e729",
   "metadata": {},
   "source": [
    "# Creamos sesión en spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef38fc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark = SparkSession.builder.getOrCreate() #Creamos la sesión\n",
    "conf = SparkConf()\\\n",
    "        .setAppName('PracticaSpark')\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffd0b83",
   "metadata": {},
   "source": [
    "# Abrimos el archivo correspondiente a junio de 2022\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e21bd510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(r\"C:/Users/j4gon/Downloads/junio2019.json\", encoding='latin1') as f:\n",
    "    data = f.readlines()\n",
    "     \n",
    "\n",
    "rdd = sc.textFile(r\"C:/Users/j4gon/Downloads/junio2019.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb1db5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FiletoDic(line):#Seleccionamos los datos que vamos a utilizar\n",
    "\n",
    "  data=json.loads(line)\n",
    "  id_bici = data['_id']\n",
    "  usuario = data['user_day_code']\n",
    "  salida = data['idunplug_station']\n",
    "  llegada = data['idplug_station']\n",
    "  tiempo = data['travel_time']\n",
    "  edad = data['ageRange']\n",
    "  \n",
    "  return usuario,id_bici,salida,llegada,tiempo,edad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fcf5735",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('e4d55deb9ac172a8d8f5f0a32599815bd51b7c8760d67e42b11adf7c0829341b',\n",
       "  {'$oid': '5cf83b752f3843a016be4e2f'},\n",
       "  90,\n",
       "  66,\n",
       "  219,\n",
       "  0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = rdd.map(FiletoDic)\n",
    "rdd.map(FiletoDic).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08f01ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 9), ('b', 17)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = sc.parallelize([('a',1),('b',4),('a',8),('b',5),('b',8)])\n",
    "#rdd_estacion_origen = rdd2.groupByKey().mapValues(lambda x: sum(list(x)))\n",
    "rdd_estacion_origen = rdd2.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "                        \n",
    "                          \n",
    "                          \n",
    "        \n",
    "\n",
    "         \n",
    "\n",
    "#rdd_estacion_destino = rdd1.map(lambda x: (x[3], 1))\\\n",
    "#                           .groupByKey()\\\n",
    "#                           .mapValues(sum)\\\n",
    "#                           .sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "rdd_estacion_origen.collect()\n",
    "#rdd_estacion_destino.take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaebd8a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('e4d55deb9ac172a8d8f5f0a32599815bd51b7c8760d67e42b11adf7c0829341b',\n",
       "  {'$oid': '5cf83b752f3843a016be4e2f'},\n",
       "  90,\n",
       "  66,\n",
       "  219,\n",
       "  0),\n",
       " ('8a0c4123e924a50a958f51985eb71aea750fb072438035f149283490cc6bfaf4',\n",
       "  {'$oid': '5cf83b762f3843a016be4e48'},\n",
       "  71,\n",
       "  136,\n",
       "  359,\n",
       "  4),\n",
       " ('a6a9c1f74a68496000542210abc4fc2eba79e2756ad5355a626632f7783dd418',\n",
       "  {'$oid': '5cf83b762f3843a016be4e4f'},\n",
       "  39,\n",
       "  38,\n",
       "  375,\n",
       "  4),\n",
       " ('5706c0bd494acc02279d532821c9666b0e506d4f81c838ef3147b2e0abbe7156',\n",
       "  {'$oid': '5cf83b762f3843a016be4e53'},\n",
       "  66,\n",
       "  90,\n",
       "  264,\n",
       "  5),\n",
       " ('eb1b6d32bd4add5d5ff91af72a38786d61075c090383a53882f33b0801eb0079',\n",
       "  {'$oid': '5cf83b762f3843a016be4e54'},\n",
       "  152,\n",
       "  166,\n",
       "  367,\n",
       "  4),\n",
       " ('c2905f6038aa9523da6bc3222c63c2ec3269c69281bf34eac161eed1ae2ba5f4',\n",
       "  {'$oid': '5cf83b762f3843a016be4e56'},\n",
       "  55,\n",
       "  53,\n",
       "  174,\n",
       "  5),\n",
       " ('f94420744ea060ac42ef3901cb251bb5b4ff63033be7bcb96d8764b968912691',\n",
       "  {'$oid': '5cf83b762f3843a016be4e59'},\n",
       "  133,\n",
       "  129,\n",
       "  308,\n",
       "  0),\n",
       " ('e75471af2ea032a327f636e28b8603c4cddab9155948fca85e82ec2f73c2bd1a',\n",
       "  {'$oid': '5cf83b762f3843a016be4e5a'},\n",
       "  153,\n",
       "  169,\n",
       "  462,\n",
       "  4),\n",
       " ('c0d73ee753773a3bfcc1483366a6ee68be9524dafbf65e81d31f4509d73304ac',\n",
       "  {'$oid': '5cf83b762f3843a016be4e5c'},\n",
       "  44,\n",
       "  129,\n",
       "  482,\n",
       "  0),\n",
       " ('47ecd557f21ca7aee6235dc015c6eaa7cea5e99e3aa8fbdfac8a308bb093f552',\n",
       "  {'$oid': '5cf83b762f3843a016be4e61'},\n",
       "  85,\n",
       "  133,\n",
       "  480,\n",
       "  4)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = rdd.map(FiletoDic)\n",
    "rdd.map(FiletoDic).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc91c777",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Obtenemos las estaciones de salida y llegada para cada usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de0bf66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def salidas(line):\n",
    "    data    = FiletoDic(line)\n",
    "    return data[0],data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd64b7fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('e4d55deb9ac172a8d8f5f0a32599815bd51b7c8760d67e42b11adf7c0829341b', 90),\n",
       " ('8a0c4123e924a50a958f51985eb71aea750fb072438035f149283490cc6bfaf4', 71),\n",
       " ('a6a9c1f74a68496000542210abc4fc2eba79e2756ad5355a626632f7783dd418', 39),\n",
       " ('5706c0bd494acc02279d532821c9666b0e506d4f81c838ef3147b2e0abbe7156', 66),\n",
       " ('eb1b6d32bd4add5d5ff91af72a38786d61075c090383a53882f33b0801eb0079', 152),\n",
       " ('c2905f6038aa9523da6bc3222c63c2ec3269c69281bf34eac161eed1ae2ba5f4', 55),\n",
       " ('f94420744ea060ac42ef3901cb251bb5b4ff63033be7bcb96d8764b968912691', 133),\n",
       " ('e75471af2ea032a327f636e28b8603c4cddab9155948fca85e82ec2f73c2bd1a', 153),\n",
       " ('c0d73ee753773a3bfcc1483366a6ee68be9524dafbf65e81d31f4509d73304ac', 44),\n",
       " ('47ecd557f21ca7aee6235dc015c6eaa7cea5e99e3aa8fbdfac8a308bb093f552', 85)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_salidas = rdd.map(salidas)\n",
    "lista_salidas.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85de947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llegadas(line):\n",
    "    data    = FiletoDic(line)\n",
    "    return data[0],data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b040c143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('e4d55deb9ac172a8d8f5f0a32599815bd51b7c8760d67e42b11adf7c0829341b', 66),\n",
       " ('8a0c4123e924a50a958f51985eb71aea750fb072438035f149283490cc6bfaf4', 136),\n",
       " ('a6a9c1f74a68496000542210abc4fc2eba79e2756ad5355a626632f7783dd418', 38),\n",
       " ('5706c0bd494acc02279d532821c9666b0e506d4f81c838ef3147b2e0abbe7156', 90),\n",
       " ('eb1b6d32bd4add5d5ff91af72a38786d61075c090383a53882f33b0801eb0079', 166),\n",
       " ('c2905f6038aa9523da6bc3222c63c2ec3269c69281bf34eac161eed1ae2ba5f4', 53),\n",
       " ('f94420744ea060ac42ef3901cb251bb5b4ff63033be7bcb96d8764b968912691', 129),\n",
       " ('e75471af2ea032a327f636e28b8603c4cddab9155948fca85e82ec2f73c2bd1a', 169),\n",
       " ('c0d73ee753773a3bfcc1483366a6ee68be9524dafbf65e81d31f4509d73304ac', 129),\n",
       " ('47ecd557f21ca7aee6235dc015c6eaa7cea5e99e3aa8fbdfac8a308bb093f552', 133)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista_llegadas = rdd.map(llegadas)\n",
    "lista_llegadas.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72df0667",
   "metadata": {},
   "source": [
    "# Ahora comprobemos aquellos usuario realizan un ciclo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f3f731",
   "metadata": {},
   "source": [
    "Un usuario realiza un ciclo si la estación de salida y de llegada coinciden, idependientemente de si en las estaciones que pare entre medias coja otra bici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b336397f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ciclo(line):\n",
    "    data = FiletoDic(line)\n",
    "    if data[2] == data[3]: \n",
    "        return (True, tiempo)\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c90d8bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bdfbb7774e97c37e95c79efa9e5f1cba2f6672dbf1f71c1faa2bc89e9e423ae0',\n",
       "  {'$oid': '5cf83b762f3843a016be4ea0'},\n",
       "  83,\n",
       "  83,\n",
       "  357,\n",
       "  0),\n",
       " ('c199289b53d6909ee2943de60baf3fd465f9ce30251683764ed30febe54b29ef',\n",
       "  {'$oid': '5cf83b762f3843a016be4eb4'},\n",
       "  48,\n",
       "  48,\n",
       "  520,\n",
       "  0)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.filter(lambda x : x[2] == x[3]).take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9f699ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "usuarios_ciclo = rdd1.filter(lambda x: x[2] == x[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9365e04",
   "metadata": {},
   "source": [
    "Obtenemos la el tiempo medio de los usuarios que realizan un ciclo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "557dada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "media_ciclo = usuarios_ciclo.map(lambda x : x[4]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9f4434e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1636.1146547502462"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "media_ciclo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65422243",
   "metadata": {},
   "source": [
    "Ahora veamos el número de que realiza cada usuario que hace un ciclo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e590ed1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 3), ('b', 1)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prueba = sc.parallelize([('a','cola'),('b','hola'),('a','colita'),('a','cola')])\n",
    "prueba.groupByKey()\\\n",
    "      .mapValues(lambda x: len(x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5675007b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('c199289b53d6909ee2943de60baf3fd465f9ce30251683764ed30febe54b29ef', 1),\n",
       " ('72c4d5ae9ffe74250e24f097428e68286a2508dfa3c5a3edb18a5778c917680b', 1),\n",
       " ('4cdb1b6ad690fd22b6ab696e30ec0df064c664907ca3b8cf9f0d03d0dbca0df0', 1),\n",
       " ('f7bd6a6fc4d1e9877a408549c6e6c5472950149a038efd55fec7e246c1d2415f', 1),\n",
       " ('a25b20b6be2e1edd62999be61293d1694a281f6a981063792d5e915f2a1165a1', 1)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_bicis_ciclo = usuarios_ciclo.map(lambda x: (x[0],x[1]))\\\n",
    "                                .groupByKey()\\\n",
    "                                .mapValues(lambda x: len(x))\n",
    "num_bicis_ciclo.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fdddee",
   "metadata": {},
   "source": [
    "Ahora veamos de los usuarios que realizan un ciclo, cuales realizan un camino, que basicamente es \n",
    "comprobar que usuarios usan la misma bici en todo el trayecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f3355d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def es_camino(lista_bicis):\n",
    "    a = lista_bicis[0]\n",
    "    camino = True\n",
    "    if len(lista_bicis) == 1:\n",
    "        return True\n",
    "    else:\n",
    "        for b in range(1,len(lista_bicis)):\n",
    "            if a != lista_bicis[b]:\n",
    "                return False\n",
    "    return camino\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "87b9de2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 18.0 failed 1 times, most recent failure: Lost task 0.0 in stage 18.0 (TID 111) (LAPTOP-1KV5MILP.home executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 830, in main\n  File \"C:\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 822, in process\n  File \"C:\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\spark-3.4.0-bin-hadoop3\\python\\pyspark\\rdd.py\", line 2830, in takeUpToNumLeft\n    yield next(iterator)\n          ^^^^^^^^^^^^^^\n  File \"C:\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\j4gon\\AppData\\Local\\Temp\\ipykernel_30476\\528160855.py\", line 3, in <lambda>\n  File \"C:\\Users\\j4gon\\AppData\\Local\\Temp\\ipykernel_30476\\1551009558.py\", line 2, in es_camino\nTypeError: 'ResultIterable' object is not subscriptable\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:179)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:179)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 830, in main\n  File \"C:\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 822, in process\n  File \"C:\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\spark-3.4.0-bin-hadoop3\\python\\pyspark\\rdd.py\", line 2830, in takeUpToNumLeft\n    yield next(iterator)\n          ^^^^^^^^^^^^^^\n  File \"C:\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\j4gon\\AppData\\Local\\Temp\\ipykernel_30476\\528160855.py\", line 3, in <lambda>\n  File \"C:\\Users\\j4gon\\AppData\\Local\\Temp\\ipykernel_30476\\1551009558.py\", line 2, in es_camino\nTypeError: 'ResultIterable' object is not subscriptable\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:179)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m usuarios_camino \u001b[38;5;241m=\u001b[39m \u001b[43musuarios_ciclo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupByKey\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mes_camino\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m----> 4\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\spark-3.4.0-bin-hadoop3\\python\\pyspark\\rdd.py:2836\u001b[0m, in \u001b[0;36mRDD.take\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   2833\u001b[0m         taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2835\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(partsScanned, \u001b[38;5;28mmin\u001b[39m(partsScanned \u001b[38;5;241m+\u001b[39m numPartsToTry, totalParts))\n\u001b[1;32m-> 2836\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeUpToNumLeft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2838\u001b[0m items \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m res\n\u001b[0;32m   2839\u001b[0m partsScanned \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m numPartsToTry\n",
      "File \u001b[1;32mC:\\spark-3.4.0-bin-hadoop3\\python\\pyspark\\context.py:2319\u001b[0m, in \u001b[0;36mSparkContext.runJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   2317\u001b[0m mappedRDD \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmapPartitions(partitionFunc)\n\u001b[0;32m   2318\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2319\u001b[0m sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunJob\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmappedRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartitions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, mappedRDD\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32mC:\\spark-3.4.0-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\spark-3.4.0-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 18.0 failed 1 times, most recent failure: Lost task 0.0 in stage 18.0 (TID 111) (LAPTOP-1KV5MILP.home executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 830, in main\n  File \"C:\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 822, in process\n  File \"C:\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\spark-3.4.0-bin-hadoop3\\python\\pyspark\\rdd.py\", line 2830, in takeUpToNumLeft\n    yield next(iterator)\n          ^^^^^^^^^^^^^^\n  File \"C:\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\j4gon\\AppData\\Local\\Temp\\ipykernel_30476\\528160855.py\", line 3, in <lambda>\n  File \"C:\\Users\\j4gon\\AppData\\Local\\Temp\\ipykernel_30476\\1551009558.py\", line 2, in es_camino\nTypeError: 'ResultIterable' object is not subscriptable\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:179)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:179)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 830, in main\n  File \"C:\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 822, in process\n  File \"C:\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\spark-3.4.0-bin-hadoop3\\python\\pyspark\\rdd.py\", line 2830, in takeUpToNumLeft\n    yield next(iterator)\n          ^^^^^^^^^^^^^^\n  File \"C:\\spark-3.4.0-bin-hadoop3\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\j4gon\\AppData\\Local\\Temp\\ipykernel_30476\\528160855.py\", line 3, in <lambda>\n  File \"C:\\Users\\j4gon\\AppData\\Local\\Temp\\ipykernel_30476\\1551009558.py\", line 2, in es_camino\nTypeError: 'ResultIterable' object is not subscriptable\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:179)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "usuarios_camino = usuarios_ciclo.map(lambda x: (x[0],x[1]))\\\n",
    "                                .groupByKey()\\\n",
    "                                .mapValues(rdd.reduce(compare_elements))\\\n",
    "                                .take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcd35ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "usuarios_camino"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
